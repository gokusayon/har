# -*- coding: utf-8 -*-
"""har.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cx7pBmYX9OldshPF9Kge_nAe9IVnqXLY
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')

# %cd '/gdrive/My Drive/ADM/Colab/LSTM'

from data_collection import *
from lstm_pickle_dump import *
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report


import tensorflow as tf
import pickle
import os

#---------------------------------Dataset and Preprocessing---------------------------------------

def show_basic_dataframe_info(dataframe):

    # Shape and how many rows and columns
    print('Number of columns in the dataframe: %i' % (dataframe.shape[1]))
    print('Number of rows in the dataframe: %i\n' % (dataframe.shape[0]))

    
def plot_axis(ax, x, y, title):
    ax.plot(x, y)
    ax.set_title(title)
    ax.xaxis.set_visible(False)
    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])
    ax.set_xlim([min(x), max(x)])
    ax.grid(True)
    
def plot_activity(activity,df):
    data = df[df['activity'] == activity][['la_x', 'la_y', 'la_z']][:200] #[:13530]
#    data = df[['la_x', 'la_x', 'la_x']][:200]
    axis = data.plot(subplots=True, figsize=(16, 12), title=activity)
    for ax in axis:
        ax.legend(loc='lower left', bbox_to_anchor=(0.5, 1))
        
def windows(data, size):
    start = 0
    while start < data.count():
        yield int(start), int(start + size)
        start += (size / 2)


def segment_signal(df,N_TIME_STEPS,step):
        
    segments = []
    labels = []
    for i in range(0, len(df) - N_TIME_STEPS, step):
        ax = df["la_x"].values[i:i+N_TIME_STEPS]
        ay = df["la_y"].values[i:i+N_TIME_STEPS]
        az = df["la_z"].values[i:i+N_TIME_STEPS]
        fox = df["gx"].values[i:i+N_TIME_STEPS]
        foy = df["gy"].values[i:i+N_TIME_STEPS]
        foz = df["gz"].values[i:i+N_TIME_STEPS]
        label = stats.mode(df['activity'][i: i + N_TIME_STEPS])[0][0]
#        segments.append([ax,ay,az])
        segments.append([ax,ay,az,fox,foy,foz])
        labels.append(label)
    return segments, labels

def variable_summaries(var):
  """Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""
  with tf.name_scope('summaries'):
    mean = tf.reduce_mean(var)
    tf.summary.scalar('mean', mean)
    with tf.name_scope('stddev'):
      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
    tf.summary.scalar('stddev', stddev)
    tf.summary.scalar('max', tf.reduce_max(var))
    tf.summary.scalar('min', tf.reduce_min(var))
    tf.summary.histogram('histogram', var)
    

#-----------------------------------------Evaluation-------------------------------
def printCM(predictions ,y_test):    
    LABELS = [ name for name in os.listdir(filepath) if os.path.isdir(os.path.join(filepath, name)) ]
    
    max_test = np.argmax(y_test, axis=1)
    max_predictions = np.argmax(predictions, axis=1)
    confusion_matrix = metrics.confusion_matrix(max_test, max_predictions)
    
    plt.figure(figsize=(6, 6))
    sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");
    plt.title("Confusion matrix")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show();
    print(classification_report(max_test, max_predictions))
    
def print_acc_loss(history):
    plt.figure(figsize=(12, 8))
    
    plt.plot(np.array(history['train_loss']), "r--", label="Train loss")
    plt.plot(np.array(history['train_acc']), "g--", label="Train accuracy")
    
    plt.plot(np.array(history['test_loss']), "r-", label="Test loss")
    plt.plot(np.array(history['test_acc']), "g-", label="Test accuracy")
    
    plt.title("Training session's progress over iterations")
    plt.legend(loc='upper right', shadow=True)
    plt.ylabel('Training Progress (Loss or Accuracy values)')
    plt.xlabel('Training Epoch')
    plt.ylim(0)
    
    plt.show()

    
def generate_random_hyperparams(lr_min, lr_max, bt_min, bt_max,hu_min, hu_max, ep_min, ep_max, l2_min, l2_max):
    '''generate random learning rate and keep probability'''
    # random search through log space for learning rate
    random_learning_rate = np.random.uniform(lr_min, lr_max)
    random_learning_rate = random_learning_rate * 10**(-3)
    
    
    random_l2loss_rate = np.random.uniform(l2_min, l2_max)
    random_l2loss_rate = random_l2loss_rate * 10**(-3)
    
    random_batch_size = np.random.uniform(bt_min, bt_max)
    hidden_units = np.random.uniform(hu_min, hu_max)
    epochs = np.random.uniform(ep_min, ep_max)
    return round(random_learning_rate, 5) ,int(random_batch_size) ,int(hidden_units), int(epochs),round(random_l2loss_rate, 5) 


def get_segment(N_TIME_STEPS, step):
    segments, labels = segment_signal(dataset, N_TIME_STEPS, step)
    np.array(segments).shape

    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, N_TIME_STEPS , N_FEATURES)
    labels = np.asarray(pd.get_dummies(labels), dtype = np.float32)
    
    X_train, X_test, y_train, y_test = train_test_split(
            reshaped_segments, labels, test_size=0.2, random_state=0)
    
    return X_train, X_test, y_train, y_test

def tesorboard_params(N_TIME_STEPS):
    log = LOGS_DIR + str(N_TIME_STEPS)
    trainPath = log + "/train/" 
    testPath = log + "/test/" 
    validationPath = log + "/validation/"
    
    if not os.path.exists(trainPath):
        os.makedirs(trainPath)
    if not os.path.exists(testPath):
        os.makedirs(testPath)
    if not os.path.exists(testPath):
        os.makedirs(testPath)
    
    train_writer = tf.summary.FileWriter(trainPath)
    test_writer = tf.summary.FileWriter(testPath)
    validation_writer = tf.summary.FileWriter(validationPath)
    
    return train_writer, test_writer, validation_writer

# Our model contains 2 fully-connected and 2 LSTM layers (stacked on each other) with 64 units each:
def model(inputs, batch_size, N_HIDDEN_UNITS, N_TIME_STEPS):
    weights = {
        'hidden': tf.Variable(tf.random_normal([N_FEATURES, N_HIDDEN_UNITS])),
        'output': tf.Variable(tf.random_normal([N_HIDDEN_UNITS, N_CLASSES]))
    }
    
    bias = {
        'hidden': tf.Variable(tf.random_normal([N_HIDDEN_UNITS], mean=1.0)),
        'output': tf.Variable(tf.random_normal([N_CLASSES]))
    }

    with tf.name_scope('inputs'):
        X = tf.transpose(inputs, [1, 0, 2])
        X = tf.reshape(X, [-1, N_FEATURES])
    
    with tf.name_scope('hidden'):
        hidden = tf.nn.relu(tf.matmul(X, weights['hidden']) + bias['hidden'])  
        hidden = tf.split(hidden, N_TIME_STEPS, 0)
    tf.summary.histogram("hidden", hidden)  


    # Stack 2 LSTM layers
    lstm = [tf.nn.rnn_cell.BasicLSTMCell(N_HIDDEN_UNITS, forget_bias=1.0) for num in range(2)]
    lstm = tf.nn.rnn_cell.MultiRNNCell(lstm)
    
    ### Run the data through the RNN layers
    with tf.name_scope("RNN_forward"):
    # Run each sequence step through the RNN with tf.nn.dynamic_rnn 
        outputs, _ = tf.nn.static_rnn(lstm, hidden, dtype=tf.float32)          

    # Get output for the last time step
    final_layer = outputs[-1]
    tf.summary.histogram("final_layer", final_layer)  
#    tf.summary.histogram('final_layer', final_layer)

    return tf.matmul(final_layer, weights['output']) + bias['output']

#----------------------------- Wrapper function for Hyperparameter Tuning -------------------------------------------
def run_model(N_TIME_STEPS,BATCH_SIZE, LEARNING_RATE,N_EPOCHS, N_HIDDEN_UNITS,L2_LOSS, log_type, early_stopping_step, save_modal):
    
    print("\n---------------------------Run_model for window size : " + str(N_TIME_STEPS) + ', step size : ' + str(N_TIME_STEPS/2) + "------------------")
    
    print("\nCreating Segments ... ")
    X_train, X_test, y_train, y_test = get_segment(N_TIME_STEPS, int(N_TIME_STEPS/2))
    
#    train_writer, test_writer, validation_writer = tesorboard_params(N_TIME_STEPS)
    
    # Early Stopping Params
    best_acc = 0
    stopping_step =0;
    
    # Building Model
    tf.reset_default_graph()
    
    print("Building Modal ... ")
    # Placeholders for our model
    X = tf.placeholder(tf.float32, [None, N_TIME_STEPS, N_FEATURES], name="input")
    Y = tf.placeholder(tf.float32, [None, N_CLASSES])
    
    # Note that we named the input tensor, that will be useful when using the model from Android. Creating the model:
    pred_Y = model(X,BATCH_SIZE, N_HIDDEN_UNITS, N_TIME_STEPS)
    tf.summary.histogram("matmul", pred_Y) 
    
    pred_softmax = tf.nn.softmax(pred_Y, name="y_")
    tf.summary.histogram("pred_softmax", pred_softmax)  
    
    
#    # Again, we must properly name the tensor from which we will obtain predictions. We will use L2 regularization and that must be noted in our loss op:
#    L2_LOSS = 0.0015
    
    l2 = L2_LOSS * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())
    
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred_Y, labels = Y)) + l2
    tf.summary.scalar('loss', loss)
    
    
    #optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)
    with tf.name_scope('train'):
      optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss)
    
    
    with tf.name_scope('accuracy'):
      with tf.name_scope('correct_prediction'):
        correct_pred = tf.equal(tf.argmax(pred_softmax, 1), tf.argmax(Y, 1))
      with tf.name_scope('accuracy'):
        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
    tf.summary.scalar('accuracy', accuracy)
    
    saver = tf.train.Saver()
    
    history = dict(train_loss=[], 
                         train_acc=[], 
                         test_loss=[], 
                         test_acc=[])
    
    sess=tf.InteractiveSession()
    merged = tf.summary.merge_all()
    sess.run(tf.global_variables_initializer())
    
    train_count = len(X_train)
    
    # Training
    for i in range(1, N_EPOCHS + 1):
        for start, end in zip(range(0, train_count, BATCH_SIZE),
                              range(BATCH_SIZE, train_count + 1,BATCH_SIZE)):
            sess.run([optimizer], feed_dict={X: X_train[start:end],
                                           Y: y_train[start:end]})
    
        _, acc_train, loss_train,summary = sess.run([pred_softmax, accuracy, loss,merged], feed_dict={
                                                X: X_train, Y: y_train})
            
        history['train_loss'].append(loss_train)
        history['train_acc'].append(acc_train)
        
#        train_writer.add_summary(summary, i)
        if (acc_train > best_acc):
            stopping_step = 0
            best_acc = acc_train
            print("Epoch: {} acc:{}".format(i,acc_train))
        else:
            stopping_step += 1
            print("Epoch: {} acc:{}".format(i,acc_train))
        if stopping_step >= early_stopping_step:
            print("Early stopping is trigger at step: {} acc:{}".format(i,acc_train))
            break;
    
    
        if i != 1 and i % 10 != 0:
            continue
    
        _, acc_test, loss_test, summary = sess.run([pred_softmax, accuracy, loss,merged], feed_dict={
                                                X: X_test, Y: y_test})
#        test_writer.add_summary(summary, i)
        
        history['test_loss'].append(loss_test)
        history['test_acc'].append(acc_test)
        print(f'epoch: {i} test accuracy: {acc_test} loss: {loss_test} | train accuracy: {acc_train} loss :{loss_train}\n')
        
         
    predictions , acc_final, loss_final, summary = sess.run([pred_softmax, accuracy, loss, merged], feed_dict={X: X_test, Y: y_test})
    printCM(predictions, y_test)
    
    print(f'\nfinal results: accuracy: {acc_final} loss: {loss_final}\n')    
    modal_summary = { 'history' : history, 'predictions' : predictions, 'true_labels' :  y_test, 'score' : { 'acc' : acc_final, 'loss' : loss_final, }}
    
    if save_modal == False:
        sess.close()
    return modal_summary, saver, sess

#----------------------------------------- Setting Initial Values -------------------------------
#Setting up initial values
random_seed = 611
plt.style.use('ggplot')

# Read form filepath (`track` data) and save csv to parent of filepath
filepath = 'D:/Bits/Sem 3/ADM/Project/python/har/Dataset/final/dataset'
#dataset = read_dataset(filepath)

# Read form saved csv
dataset = pd.read_csv("D:/Bits/Sem 3/ADM/Project/python/har/Dataset/final/colab_dataset.csv")

N_FEATURES = 6
N_CLASSES = 6
PICKLE_DIR = 'pickle/'
LOGS_DIR = './logs/'

#----------------------------- Tuned Parameters -------------------------------------------

#----------------------------- Hyper Parameter Tuning -------------------------------------------
#def generate_random_hyperparams(lr_min, lr_max, bt_min, bt_max,hu_min, hu_max, ep_min, ep_max, l2_min, l2_max):

# The number of steps within one time segment (window)
window_list  = [50, 80, 120, 160, 200]
performance_records = {}

for window in window_list:
    dump_path = PICKLE_DIR + "/" + str(window)
    for i in range(5): # random search hyper-parameter space 10 times for best epochs and hidden units with fixed lr and bs
    
        print("\n==============================================================================================")
        learning_rate, batch_size, hidden_units, epochs , l2_loss= generate_random_hyperparams(3, 1, 100, 500, 20, 70, 20, 50, 3, 1) 
        
        params = {'batch_size' : batch_size, 
                       'learning_rate' : learning_rate, 
                       'epochs' : epochs,
                       'hidden_units' : hidden_units,
                       'l2_loss' : l2_loss }
        
        print(f'---------- Model : {str(i)} ---------------')    
        print(f'\nlearning_rate: {learning_rate}, batch_size: {batch_size}, hidden_units: {hidden_units}, epochs: {epochs}, l2_loss: {l2_loss} \n')    
        
    #    summary = [run_model(window,batch_size, learning_rate,epochs, hidden_units,l2_loss, "epochs_hu", 2, False) for window in window_list]   
        summary, saver, sess = run_model(window,batch_size,learning_rate,epochs, hidden_units,l2_loss, "epochs_hu", 3, False)
        performance_records[(batch_size,learning_rate,epochs,hidden_units,l2_loss)] = summary
        sess.close()    
    
    if not os.path.exists(dump_path):
        os.makedirs(dump_path)
    
    print("Dumping Performance records ..")
    pickle.dump(performance_records, open(dump_path + "/performance_records.p", "wb"))

results = get_tuning_result(PICKLE_DIR)

#------------------------------------Exporting the model------------------------------------
# =============================================================================
# # Storing model to disk
# DUMP_DIR = LOGS_DIR + "/checkpoint/"
# 
# if not os.path.exists(DUMP_DIR):
#     os.makedirs(DUMP_DIR)
# 
# print("Dumping predictions and history ..")
# pickle.dump(predictions, open(DUMP_DIR + "predictions.p", "wb"))
# pickle.dump(history, open(DUMP_DIR + "history.p", "wb"))
# 
# print("Wrtiting graph : har.pbtxt ..")
# tf.train.write_graph(sess.graph_def, '.', DUMP_DIR + 'har.pbtxt')  
# 
# print("Saving checkpoint and closing session ..")
# saver.save(sess, save_path = DUMP_DIR + "har.ckpt")
# sess.close()
# 
# print("Load checkpoints and freeze graph for later use ..")
# 
# from tensorflow.python.tools import freeze_graph
# 
# MODEL_NAME = 'har'
# input_graph_path =  DUMP_DIR +  MODEL_NAME +'.pbtxt'
# checkpoint_path =  DUMP_DIR + MODEL_NAME + '.ckpt'
# restore_op_name = "save/restore_all"
# filename_tensor_name = "save/Const:0"
# output_frozen_graph =DUMP_DIR + MODEL_NAME +'.pb'
# 
# freeze_graph.freeze_graph(input_graph_path, input_saver="",
#                           input_binary=False, input_checkpoint=checkpoint_path, 
#                           output_node_names="y_", restore_op_name="save/restore_all",
#                           filename_tensor_name="save/Const:0", 
#                           output_graph=output_frozen_graph, clear_devices=True, initializer_nodes="")
# =============================================================================

#------------------------------------ Validate Tensors ------------------------------------
# =============================================================================
# print("Load frozen graph and print tensors ..\n")
# def load_graph(frozen_graph_filename):
#     # We load the protobuf file from the disk and parse it to retrieve the 
#     # unserialized graph_def
#     with tf.gfile.GFile(frozen_graph_filename, "rb") as f:
#         graph_def = tf.GraphDef()
#         graph_def.ParseFromString(f.read())
# 
#     # Then, we import the graph_def into a new Graph and return it 
#     with tf.Graph().as_default() as graph:
#         # The name var will prefix every op/nodes in your graph
#         # Since we load everything in a new graph, this is not needed
#         tf.import_graph_def(graph_def, name="prefix")
#     return graph
# 
# graph = load_graph(output_frozen_graph)
# 
# for op in graph.get_operations(): 
#         if "y_" in op.name or "input" in op.name:
#             print(op.name, " : " ,op.values())
# =============================================================================